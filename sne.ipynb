{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 確率的隣接埋め込みモデル SNE\n",
    "\n",
    "[SNE](http://localhost:8888/notebooks/sne.ipynb)\n",
    "\n",
    "確率的隣接埋め込みモデル Stochastic Neighbor Embedding models とは，\n",
    "2002年に提案された視覚化手法である。\n",
    "Hinton and Roweis (2002)\n",
    "\n",
    "対象 \\\\(i\\\\) と対象 \\\\(j\\\\) との距離 \\\\(d_{ij}\\\\) とする。他の手法と異なり\n",
    "対象間の距離が確率変動すると考える。対象 \\\\(i\\\\) と \\\\(j\\\\) との距離を取る確率を \\\\(p_{ij}\\\\) とすれば\n",
    "\n",
    "$$\n",
    "p_{ij}=\\frac{\\exp\\left(-d_{ij}^2\\right)}{\\sum_{k\\ne i}\\exp\\left(-d_{ik}^2\\right)}\n",
    "$$\n",
    "\n",
    "距離 \\\\(d_{ij}\\\\) は，\n",
    "\n",
    "$$\n",
    "d_{ij}^2 =\\frac{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "\n",
    "一方，次元圧縮後の対象 \\\\(i\\\\) の座標を $y_i$ とすれば，変換後の距離 \\\\(q_{ij}\\\\) は\n",
    "\n",
    "$$\n",
    "q_{ij}=\\frac{\\exp\\left(-\\left\\|\\mathbf{y}_{i}-\\mathbf{y}_{j}\\right\\|^2\\right)}\n",
    "{\\sum_{k\\ne i}\\exp\\left(-\\left\\|\\mathbf{y}_{i}-\\mathbf{y}_{k}\\right\\|^2\\right)}\n",
    "$$\n",
    "\n",
    "最小化すべき目標関数はカルバック=ライブラーの情報量 (Kallback Leiblier divergence) の総和，\n",
    "\n",
    "$$\n",
    "C=\\sum_i\\sum_jp_{ij}\\log\\frac{p_{ij}}{q_{ij}}=\\sum_{i}KL\\left(P_{i}||Q_i\\right)\n",
    "$$\n",
    "\n",
    "として定義できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
